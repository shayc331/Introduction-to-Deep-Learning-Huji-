# -*- coding: utf-8 -*-
"""DL_ex1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W1KOt4FLohEpGltcGW2wdYmqukjAGE-x
"""

# Deep Learning ex1
# Shay Cohen 314997388
# Itay Chachy 208489732

from google.colab import drive
drive.mount('/content/gdrive')

# Imports
import pandas as pd
from sklearn.metrics import confusion_matrix
import seaborn as sns
from sklearn.model_selection import train_test_split
from torch.utils.data import WeightedRandomSampler
import torch
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np
import torch.nn as nn
import torch.nn.functional as F
from time import time
from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder()

amino_seq_q6 = "MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSSVLHSTQDLFLPFFSNVTWFHAIHVSG" \
               "TNGTKRFDNPVLPFNDGVYFASTEKSNIIRGWIFGTTLDSKTQSLLIVNNATNVVIKVCEFQFCNDPFLGVY" \
               "YHKNNKSWMESEFRVYSSANNCTFEYVSQPFLMDLEGKQGNFKNLREFVFKNIDGYFKIYSKHTPINLVRDL" \
               "PQGFSALEPLVDLPIGINITRFQTLLALHRSYLTPGDSSSGWTAGAAAYYVGYLQPRTFLLKYNENGTITDA" \
               "VDCALDPLSETKCTLKSFTVEKGIYQTSNFRVQPTESIVRFPNITNLCPFGEVFNATRFASVYAWNRKRISN" \
               "CVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGC" \
               "VIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQPTNGVG" \
               "YQPYRVVVLSFELLHAPATVCGPKKSTNLVKNKCVNFNFNGLTGTGVLTESNKKFLPFQQFGRDIADTTDAV" \
               "RDPQTLEILDITPCSFGGVSVITPGTNTSNQVAVLYQDVNCTEVPVAIHADQLTPTWRVYSTGSNVFQTRAG" \
               "CLIGAEHVNNSYECDIPIGAGICASYQTQTNSPRRARSVASQSIIAYTMSLGAENSVAYSNNSIAIPTNFTI" \
               "SVTTEILPVSMTKTSVDCTMYICGDSTECSNLLLQYGSFCTQLNRALTGIAVEQDKNTQEVFAQVKQIYKTP" \
               "PIKDFGGFNFSQILPDPSKPSKRSFIEDLLFNKVTLADAGFIKQYGDCLGDIAARDLICAQKFNGLTVLPPL" \
               "LTDEMIAQYTSALLAGTITSGWTFGAGAALQIPFAMQMAYRFNGIGVTQNVLYENQKLIANQFNSAIGKIQD" \
               "SLSSTASALGKLQDVVNQNAQALNTLVKQLSSNFGAISSVLNDILSRLDKVEAEVQIDRLITGRLQSLQTYV" \
               "TQQLIRAAEIRASANLAATKMSECVLGQSKRVDFCGKGYHLMSFPQSAPHGVVFLHVTYVPAQEKNFTTAPA" \
               "ICHDGKAHFPREGVFVSNGTHWFVTQRNFYEPQIITTDNTFVSGNCDVVIGIVNNTVYDPLQPELDSFKEEL" \
               "DKYFKNHTSPDVDLGDISGINASVVNIQKEIDRLNEVAKNLNESLIDLQELGKYEQYIKWPWYIWLGFIAGLI" \
               "AIVMVTIMLCCMTSCCSCLKGCCSCGSCCKFDEDDSEPVLKGVKLHYT"

# For questions 6, 7
pepides = []
for i in range(len(amino_seq_q6) - 8):
  pepides.append(amino_seq_q6[i : i + 9])
pepides = np.array(pepides)
pepides = pd.DataFrame(pepides)

pepides = pepides.applymap(lambda seq: list(seq))
pepides = pd.DataFrame(pepides[0].to_list())

# Load sick file

sick = np.loadtxt("/content/gdrive/MyDrive/Deep Learning/ex1/pos_A0201.txt", dtype=np.str)
sick = pd.DataFrame(sick)
 
sick = sick.applymap(lambda seq: list(seq))
sick = pd.DataFrame(sick[0].to_list())

# oversampling
sick = pd.concat([sick, sick])

# Load healthy file

healthy = np.loadtxt("/content/gdrive/MyDrive/Deep Learning/ex1/neg_A0201.txt", dtype=np.str)
healthy = pd.DataFrame(healthy)

healthy = healthy.applymap(lambda seq: list(seq))
healthy = pd.DataFrame(healthy[0].to_list())

# Combine data and create OneHotEncoder

data = pd.concat([sick, healthy, pepides], axis=0)
data = ohe.fit_transform(data).toarray().reshape(-1, 180)
q9_data = data[len(sick) + len(healthy):]
data = data[:len(sick) + len(healthy)]
data_label = np.zeros((len(data), 1))

data_label[:len(sick) - 1] = 1

data = np.hstack((data, data_label))


class Dataset():
  def __init__(self, data):
    self._x = torch.tensor(data[:, :-1]).type(torch.FloatTensor)
    self._y = torch.tensor(data[:, -1]).reshape(len(self._x) ,1).type(torch.FloatTensor)

  def __getitem__(self, key):
    return self._x[key], self._y[key]

  def __len__(self):
    return len(self._x)

  def get_y(self):
    return self._y.numpy()


# Splits data


training_data, testing_data = train_test_split(data, test_size=0.1, random_state=25)

training_data = Dataset(training_data)
testing_data = Dataset(testing_data)

training_y = training_data.get_y()

# Generate samples weights

class_sample_count = np.array([len(np.where(training_y == t)[0]) for t in np.unique(training_y)])
weight = 1 / class_sample_count
samples_weight = np.array([weight[int(t)] for t in training_y])
samples_weight = torch.from_numpy(samples_weight)

sampler = WeightedRandomSampler(samples_weight, len(samples_weight))

Batch_size = 64

train_dataloader = torch.utils.data.DataLoader(dataset=training_data, batch_size=Batch_size, sampler=sampler)
full_train_dataloader = torch.utils.data.DataLoader(dataset=training_data, batch_size=len(training_data))
test_dataloader = torch.utils.data.DataLoader(dataset=testing_data, batch_size=len(testing_data))

# Network

class Net(nn.Module):
  def __init__(self):
    super(Net, self).__init__()
    self.fc1 = nn.Linear(180, 60)
    self.fc2 = nn.BatchNorm1d(60)
    self.fc3 = nn.Linear(60, 20)
    self.fc4 = nn.BatchNorm1d(20)
    self.fc5 = nn.Linear(20, 1)

  def forward(self, x):
    if len(x.shape) > 1:  # Due to batch-norm
      x = F.relu(self.fc2(self.fc1(x)))
      x = F.relu(self.fc4(self.fc3(x)))
    else:
      x = F.relu(self.fc1(x))
      x = F.relu(self.fc3(x))
    x = torch.sigmoid(self.fc5(x))
    return x

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

net = Net().to(device)

criterion = nn.BCELoss()
optimizer = optim.Adam(net.parameters(), lr=0.001)


def roundPrediction(y):
  y_ = np.zeros(y.size)
  y_[y > 0.5] = 1
  return y_


class PlotData():
  def __init__(self, n, title):
    self.n = n
    self.title = title


def plotConfusionMatrix(labels, predictions, plotData):
  cm = confusion_matrix(labels, predictions)
  ax= plt.subplot()

  # annot=True to annotate cells, ftm='g' to disable scientific notation
  sns.heatmap(cm, annot=True, fmt='g', ax=ax)

  ax.set_xlabel('Predicted labels')
  ax.set_ylabel('True labels')
  ax.set_title(f'{plotData.title} - Confusion Matrix - epoch {plotData.n}')
  ax.xaxis.set_ticklabels(['helthy', 'sick'])
  ax.yaxis.set_ticklabels(['helthy', 'sick'])


def calculateAndPlotMeasurements(labels, predictions, plotData):
  p = np.count_nonzero(labels == 1)
  n = labels.size - p

  tn, fp, fn, tp = confusion_matrix(labels, predictions).ravel()

  accuracy = (tn + tp) / (p + n)
  error_rate = (fn + fp) / (p + n)
  TPR = tp / p
  FNR = fp / n

  df = pd.DataFrame({'Measurement':["Accuracy", "Error_rate", "TPR", "FNR"], 'Rate':[accuracy, error_rate, TPR, FNR]})

  df.plot.bar(x=f'Measurement', y='Rate', rot=0, title=f"{plotData.title} - epoch {plotData.n}")


def plotMetrics(loader, plotData):
    data = next(iter(loader))
    data = [item.to(device) for item in data]
    amino_seq, labels = data
    outputs = net(amino_seq)
    outputs = outputs.cpu().data.numpy().reshape(-1)
    labels = labels.cpu().data.numpy().reshape(-1)
    predictions = roundPrediction(outputs)
    plotConfusionMatrix(labels, predictions, plotData)
    plt.show()
    calculateAndPlotMeasurements(labels, predictions, plotData)
    plt.show()

# Training

start_time=time()
epoch_size = 15

for epoch in range(1, epoch_size + 1):
    for i, batch in enumerate(train_dataloader):
      batch = [item.to(device) for item in batch]
      amino_seq, labels = batch
      optimizer.zero_grad()
      outputs = net(amino_seq)
      train_loss = criterion(outputs, labels)
      train_loss.backward()
      optimizer.step()
    print(f'Epoch {epoch} is over')

    if epoch in {1, 5, 10, 15}:
      plotMetrics(full_train_dataloader, PlotData(epoch, 'Training')) 
      plotMetrics(test_dataloader, PlotData(epoch, 'Testing'))     

print("Time for training using PyTorch is %f" %(time()-start_time))


class CovidSeq():
  def __init__(self, seq, p):
    self.seq = self._init_seq(seq)
    self.p = p

  def _init_seq(self, seq):
    s = ""
    for c in seq:
      s += c
    return s

  def __str__(self):
    return f'Pepides- {self.seq} : probability according to model- {self.p}\n'


q9_data = torch.tensor(q9_data).type(torch.FloatTensor)


def find_top_strongest_pepides():
  net.to('cpu')
  outputs = net(q9_data).reshape(-1)
  max_five = torch.topk(outputs, 5)
  outputs = outputs.detach().numpy()
  results = list()
  for i in max_five[1]:
    results.append(CovidSeq(pepides.iloc[int(i)], outputs[int(i)]))
  return results


r = find_top_strongest_pepides()

for t in r:
  print(t)

def random_peptide():
  w = torch.zeros(9, 20)
  for i in range(9):
    w[i, np.random.randint(0, 20)] = 1
  return w.reshape(180).to(device)


net.requires_grad_(False)
w = torch.nn.Parameter(random_peptide(), requires_grad=True)
optimizer = optim.Adam([w], lr=0.001)
y = torch.ones(1, device=device)

num_steps = 1000
net.to(device)

for step in range(1, num_steps + 1):
    loss = criterion(net(w), y)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()

w.requires_grad = False
prediction = net(w)

decoded = ohe.inverse_transform(w.to('cpu').reshape(1, 180))
print(CovidSeq(list(decoded), prediction[0]))
